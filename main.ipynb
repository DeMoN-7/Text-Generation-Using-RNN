{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08823029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761548e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9d9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess text\n",
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546c7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create character mapping\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84932c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare dataset\n",
    "seq_length = 100\n",
    "step = 1\n",
    "\n",
    "sequences = []\n",
    "next_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0393e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(text) - seq_length, step):\n",
    "    sequences.append(text[i:i+seq_length])\n",
    "    next_chars.append(text[i+seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a58a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer encoding\n",
    "X = [[char2idx[c] for c in seq] for seq in sequences]\n",
    "y = [char2idx[c] for c in next_chars]\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06fb42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reduce dataset size to test faster\n",
    "X = X[:1000000]\n",
    "y = y[:1000000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2737dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset and DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0b3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define the model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=256):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Only last time step\n",
    "        return out, hidden\n",
    "# Clip gradients to prevent explosion\n",
    "\n",
    "model = CharRNN(vocab_size).to(device)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e2ac759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 2096.5330\n",
      "Epoch 2/500, Loss: 1709.7689\n",
      "Epoch 3/500, Loss: 1582.1136\n",
      "Epoch 4/500, Loss: 1502.0764\n",
      "Epoch 5/500, Loss: 1447.0344\n",
      "Epoch 6/500, Loss: 1405.9842\n",
      "Epoch 7/500, Loss: 1372.4659\n",
      "Epoch 8/500, Loss: 1345.4979\n",
      "Epoch 9/500, Loss: 1320.5332\n",
      "Epoch 10/500, Loss: 1300.5979\n",
      "Epoch 11/500, Loss: 1281.4838\n",
      "Epoch 12/500, Loss: 1265.1940\n",
      "Epoch 13/500, Loss: 1250.0177\n",
      "Epoch 14/500, Loss: 1235.6572\n",
      "Epoch 15/500, Loss: 1222.2782\n",
      "Epoch 16/500, Loss: 1209.8380\n",
      "Epoch 17/500, Loss: 1198.7786\n",
      "Epoch 18/500, Loss: 1187.5709\n",
      "Epoch 19/500, Loss: 1178.4263\n",
      "Epoch 20/500, Loss: 1168.2863\n",
      "Epoch 21/500, Loss: 1158.6445\n",
      "Epoch 22/500, Loss: 1149.2485\n",
      "Epoch 23/500, Loss: 1140.9859\n",
      "Epoch 24/500, Loss: 1132.9825\n",
      "Epoch 25/500, Loss: 1124.9031\n",
      "Epoch 26/500, Loss: 1116.9243\n",
      "Epoch 27/500, Loss: 1109.8027\n",
      "Epoch 28/500, Loss: 1102.2315\n",
      "Epoch 29/500, Loss: 1095.0580\n",
      "Epoch 30/500, Loss: 1087.7547\n",
      "Epoch 31/500, Loss: 1081.3802\n",
      "Epoch 32/500, Loss: 1074.9526\n",
      "Epoch 33/500, Loss: 1068.3567\n",
      "Epoch 34/500, Loss: 1061.6815\n",
      "Epoch 35/500, Loss: 1055.6597\n",
      "Epoch 36/500, Loss: 1049.9118\n",
      "Epoch 37/500, Loss: 1043.6308\n",
      "Epoch 38/500, Loss: 1037.9122\n",
      "Epoch 39/500, Loss: 1031.9296\n",
      "Epoch 40/500, Loss: 1025.9692\n",
      "Epoch 41/500, Loss: 1020.3585\n",
      "Epoch 42/500, Loss: 1015.2423\n",
      "Epoch 43/500, Loss: 1009.9643\n",
      "Epoch 44/500, Loss: 1004.5029\n",
      "Epoch 45/500, Loss: 999.0651\n",
      "Epoch 46/500, Loss: 994.0919\n",
      "Epoch 47/500, Loss: 988.4712\n",
      "Epoch 48/500, Loss: 983.4506\n",
      "Epoch 49/500, Loss: 978.5248\n",
      "Epoch 50/500, Loss: 973.3363\n",
      "Epoch 51/500, Loss: 968.3326\n",
      "Epoch 52/500, Loss: 963.1613\n",
      "Epoch 53/500, Loss: 958.1887\n",
      "Epoch 54/500, Loss: 953.5432\n",
      "Epoch 55/500, Loss: 948.4152\n",
      "Epoch 56/500, Loss: 943.4097\n",
      "Epoch 57/500, Loss: 939.0730\n",
      "Epoch 58/500, Loss: 933.8524\n",
      "Epoch 59/500, Loss: 928.8606\n",
      "Epoch 60/500, Loss: 924.6262\n",
      "Epoch 61/500, Loss: 920.4590\n",
      "Epoch 62/500, Loss: 914.9029\n",
      "Epoch 63/500, Loss: 910.7132\n",
      "Epoch 64/500, Loss: 905.6953\n",
      "Epoch 65/500, Loss: 901.1016\n",
      "Epoch 66/500, Loss: 896.5283\n",
      "Epoch 67/500, Loss: 892.3650\n",
      "Epoch 68/500, Loss: 887.3998\n",
      "Epoch 69/500, Loss: 882.9018\n",
      "Epoch 70/500, Loss: 879.1212\n",
      "Epoch 71/500, Loss: 874.3916\n",
      "Epoch 72/500, Loss: 868.7486\n",
      "Epoch 73/500, Loss: 864.6270\n",
      "Epoch 74/500, Loss: 860.2144\n",
      "Epoch 75/500, Loss: 856.2090\n",
      "Epoch 76/500, Loss: 851.4782\n",
      "Epoch 77/500, Loss: 846.9704\n",
      "Epoch 78/500, Loss: 842.2266\n",
      "Epoch 79/500, Loss: 837.3562\n",
      "Epoch 80/500, Loss: 832.5688\n",
      "Epoch 81/500, Loss: 828.5695\n",
      "Epoch 82/500, Loss: 824.5128\n",
      "Epoch 83/500, Loss: 819.1234\n",
      "Epoch 84/500, Loss: 814.6674\n",
      "Epoch 85/500, Loss: 811.6264\n",
      "Epoch 86/500, Loss: 805.7412\n",
      "Epoch 87/500, Loss: 801.9229\n",
      "Epoch 88/500, Loss: 797.3164\n",
      "Epoch 89/500, Loss: 792.2173\n",
      "Epoch 90/500, Loss: 788.1111\n",
      "Epoch 91/500, Loss: 783.2398\n",
      "Epoch 92/500, Loss: 778.8506\n",
      "Epoch 93/500, Loss: 774.1932\n",
      "Epoch 94/500, Loss: 770.5785\n",
      "Epoch 95/500, Loss: 765.4083\n",
      "Epoch 96/500, Loss: 761.0105\n",
      "Epoch 97/500, Loss: 755.9788\n",
      "Epoch 98/500, Loss: 751.8079\n",
      "Epoch 99/500, Loss: 747.8029\n",
      "Epoch 100/500, Loss: 743.1109\n",
      "Epoch 101/500, Loss: 738.1898\n",
      "Epoch 102/500, Loss: 736.7095\n",
      "Epoch 103/500, Loss: 732.4937\n",
      "Epoch 104/500, Loss: 724.4783\n",
      "Epoch 105/500, Loss: 719.8262\n",
      "Epoch 106/500, Loss: 717.3884\n",
      "Epoch 107/500, Loss: 711.8004\n",
      "Epoch 108/500, Loss: 707.2717\n",
      "Epoch 109/500, Loss: 703.6595\n",
      "Epoch 110/500, Loss: 698.4745\n",
      "Epoch 111/500, Loss: 694.1715\n",
      "Epoch 112/500, Loss: 689.8783\n",
      "Epoch 113/500, Loss: 685.5089\n",
      "Epoch 114/500, Loss: 680.7998\n",
      "Epoch 115/500, Loss: 676.4645\n",
      "Epoch 116/500, Loss: 672.4673\n",
      "Epoch 117/500, Loss: 668.0617\n",
      "Epoch 118/500, Loss: 664.0236\n",
      "Epoch 119/500, Loss: 659.2044\n",
      "Epoch 120/500, Loss: 653.9678\n",
      "Epoch 121/500, Loss: 649.6431\n",
      "Epoch 122/500, Loss: 646.1732\n",
      "Epoch 123/500, Loss: 642.5505\n",
      "Epoch 124/500, Loss: 637.4411\n",
      "Epoch 125/500, Loss: 631.5425\n",
      "Epoch 126/500, Loss: 628.4411\n",
      "Epoch 127/500, Loss: 623.9279\n",
      "Epoch 128/500, Loss: 620.2335\n",
      "Epoch 129/500, Loss: 615.9961\n",
      "Epoch 130/500, Loss: 612.5901\n",
      "Epoch 131/500, Loss: 606.8246\n",
      "Epoch 132/500, Loss: 602.2052\n",
      "Epoch 133/500, Loss: 598.4593\n",
      "Epoch 134/500, Loss: 593.7980\n",
      "Epoch 135/500, Loss: 589.6414\n",
      "Epoch 136/500, Loss: 585.5891\n",
      "Epoch 137/500, Loss: 581.4715\n",
      "Epoch 138/500, Loss: 576.4581\n",
      "Epoch 139/500, Loss: 572.9548\n",
      "Epoch 140/500, Loss: 568.2932\n",
      "Epoch 141/500, Loss: 564.4576\n",
      "Epoch 142/500, Loss: 559.9264\n",
      "Epoch 143/500, Loss: 556.5951\n",
      "Epoch 144/500, Loss: 551.3464\n",
      "Epoch 145/500, Loss: 546.6627\n",
      "Epoch 146/500, Loss: 545.3366\n",
      "Epoch 147/500, Loss: 539.2356\n",
      "Epoch 148/500, Loss: 534.0451\n",
      "Epoch 149/500, Loss: 531.5320\n",
      "Epoch 150/500, Loss: 526.8730\n",
      "Epoch 151/500, Loss: 523.7981\n",
      "Epoch 152/500, Loss: 518.6085\n",
      "Epoch 153/500, Loss: 515.5800\n",
      "Epoch 154/500, Loss: 510.9681\n",
      "Epoch 155/500, Loss: 508.3101\n",
      "Epoch 156/500, Loss: 503.6041\n",
      "Epoch 157/500, Loss: 498.7905\n",
      "Epoch 158/500, Loss: 494.2194\n",
      "Epoch 159/500, Loss: 492.4834\n",
      "Epoch 160/500, Loss: 486.7901\n",
      "Epoch 161/500, Loss: 481.8657\n",
      "Epoch 162/500, Loss: 479.1270\n",
      "Epoch 163/500, Loss: 475.2453\n",
      "Epoch 164/500, Loss: 471.3305\n",
      "Epoch 165/500, Loss: 467.1524\n",
      "Epoch 166/500, Loss: 463.5120\n",
      "Epoch 167/500, Loss: 459.1568\n",
      "Epoch 168/500, Loss: 457.0691\n",
      "Epoch 169/500, Loss: 451.5707\n",
      "Epoch 170/500, Loss: 449.0689\n",
      "Epoch 171/500, Loss: 444.6486\n",
      "Epoch 172/500, Loss: 443.1960\n",
      "Epoch 173/500, Loss: 438.6375\n",
      "Epoch 174/500, Loss: 433.7716\n",
      "Epoch 175/500, Loss: 432.7106\n",
      "Epoch 176/500, Loss: 427.3914\n",
      "Epoch 177/500, Loss: 420.9664\n",
      "Epoch 178/500, Loss: 420.4443\n",
      "Epoch 179/500, Loss: 415.2392\n",
      "Epoch 180/500, Loss: 414.6932\n",
      "Epoch 181/500, Loss: 410.8935\n",
      "Epoch 182/500, Loss: 406.3682\n",
      "Epoch 183/500, Loss: 405.4424\n",
      "Epoch 184/500, Loss: 400.4076\n",
      "Epoch 185/500, Loss: 394.1839\n",
      "Epoch 186/500, Loss: 390.0991\n",
      "Epoch 187/500, Loss: 388.9270\n",
      "Epoch 188/500, Loss: 384.5700\n",
      "Epoch 189/500, Loss: 381.0085\n",
      "Epoch 190/500, Loss: 378.9950\n",
      "Epoch 191/500, Loss: 377.5915\n",
      "Epoch 192/500, Loss: 376.2472\n",
      "Epoch 193/500, Loss: 367.0625\n",
      "Epoch 194/500, Loss: 372.1921\n",
      "Epoch 195/500, Loss: 360.8355\n",
      "Epoch 196/500, Loss: 361.6294\n",
      "Epoch 197/500, Loss: 357.8281\n",
      "Epoch 198/500, Loss: 353.1128\n",
      "Epoch 199/500, Loss: 350.0616\n",
      "Epoch 200/500, Loss: 345.3359\n",
      "Epoch 201/500, Loss: 345.4240\n",
      "Epoch 202/500, Loss: 339.2567\n",
      "Epoch 203/500, Loss: 339.3884\n",
      "Epoch 204/500, Loss: 336.8287\n",
      "Epoch 205/500, Loss: 330.5067\n",
      "Epoch 206/500, Loss: 328.5127\n",
      "Epoch 207/500, Loss: 326.4277\n",
      "Epoch 208/500, Loss: 324.9995\n",
      "Epoch 209/500, Loss: 319.2968\n",
      "Epoch 210/500, Loss: 316.1077\n",
      "Epoch 211/500, Loss: 312.9681\n",
      "Epoch 212/500, Loss: 309.4176\n",
      "Epoch 213/500, Loss: 309.5769\n",
      "Epoch 214/500, Loss: 308.7041\n",
      "Epoch 215/500, Loss: 301.5784\n",
      "Epoch 216/500, Loss: 303.4426\n",
      "Epoch 217/500, Loss: 297.9275\n",
      "Epoch 218/500, Loss: 292.4385\n",
      "Epoch 219/500, Loss: 293.1392\n",
      "Epoch 220/500, Loss: 288.9915\n",
      "Epoch 221/500, Loss: 285.2572\n",
      "Epoch 222/500, Loss: 281.4226\n",
      "Epoch 223/500, Loss: 282.3498\n",
      "Epoch 224/500, Loss: 283.5873\n",
      "Epoch 225/500, Loss: 276.7379\n",
      "Epoch 226/500, Loss: 276.9224\n",
      "Epoch 227/500, Loss: 269.9790\n",
      "Epoch 228/500, Loss: 267.4112\n",
      "Epoch 229/500, Loss: 266.8115\n",
      "Epoch 230/500, Loss: 263.5370\n",
      "Epoch 231/500, Loss: 262.8212\n",
      "Epoch 232/500, Loss: 265.9135\n",
      "Epoch 233/500, Loss: 259.1343\n",
      "Epoch 234/500, Loss: 251.8307\n",
      "Epoch 235/500, Loss: 253.0963\n",
      "Epoch 236/500, Loss: 250.4427\n",
      "Epoch 237/500, Loss: 248.6087\n",
      "Epoch 238/500, Loss: 244.3214\n",
      "Epoch 239/500, Loss: 243.6204\n",
      "Epoch 240/500, Loss: 240.0198\n",
      "Epoch 241/500, Loss: 244.8436\n",
      "Epoch 242/500, Loss: 235.5996\n",
      "Epoch 243/500, Loss: 235.5536\n",
      "Epoch 244/500, Loss: 239.9000\n",
      "Epoch 245/500, Loss: 232.2222\n",
      "Epoch 246/500, Loss: 224.9600\n",
      "Epoch 247/500, Loss: 227.1165\n",
      "Epoch 248/500, Loss: 224.8613\n",
      "Epoch 249/500, Loss: 225.4960\n",
      "Epoch 250/500, Loss: 219.9688\n",
      "Epoch 251/500, Loss: 221.7464\n",
      "Epoch 252/500, Loss: 217.6434\n",
      "Epoch 253/500, Loss: 214.8750\n",
      "Epoch 254/500, Loss: 213.8633\n",
      "Epoch 255/500, Loss: 213.7663\n",
      "Epoch 256/500, Loss: 209.6959\n",
      "Epoch 257/500, Loss: 204.3846\n",
      "Epoch 258/500, Loss: 206.9734\n",
      "Epoch 259/500, Loss: 207.5700\n",
      "Epoch 260/500, Loss: 199.5559\n",
      "Epoch 261/500, Loss: 196.7508\n",
      "Epoch 262/500, Loss: 202.2459\n",
      "Epoch 263/500, Loss: 198.8024\n",
      "Epoch 264/500, Loss: 197.1795\n",
      "Epoch 265/500, Loss: 195.1880\n",
      "Epoch 266/500, Loss: 194.1962\n",
      "Epoch 267/500, Loss: 187.5737\n",
      "Epoch 268/500, Loss: 188.3851\n",
      "Epoch 269/500, Loss: 191.0983\n",
      "Epoch 270/500, Loss: 188.1990\n",
      "Epoch 271/500, Loss: 183.2583\n",
      "Epoch 272/500, Loss: 183.8857\n",
      "Epoch 273/500, Loss: 176.3643\n",
      "Epoch 274/500, Loss: 184.6061\n",
      "Epoch 275/500, Loss: 220.4767\n",
      "Epoch 276/500, Loss: 187.7589\n",
      "Epoch 277/500, Loss: 165.7759\n",
      "Epoch 278/500, Loss: 167.0983\n",
      "Epoch 279/500, Loss: 174.7697\n",
      "Epoch 280/500, Loss: 175.5952\n",
      "Epoch 281/500, Loss: 169.9123\n",
      "Epoch 282/500, Loss: 171.2434\n",
      "Epoch 283/500, Loss: 174.2584\n",
      "Epoch 284/500, Loss: 175.0726\n",
      "Epoch 285/500, Loss: 169.5138\n",
      "Epoch 286/500, Loss: 159.1926\n",
      "Epoch 287/500, Loss: 162.0901\n",
      "Epoch 288/500, Loss: 166.0813\n",
      "Epoch 289/500, Loss: 177.4234\n",
      "Epoch 290/500, Loss: 149.9158\n",
      "Epoch 291/500, Loss: 160.7590\n",
      "Epoch 292/500, Loss: 155.1679\n",
      "Epoch 293/500, Loss: 156.0763\n",
      "Epoch 294/500, Loss: 154.8860\n",
      "Epoch 295/500, Loss: 153.4714\n",
      "Epoch 296/500, Loss: 151.8873\n",
      "Epoch 297/500, Loss: 154.4159\n",
      "Epoch 298/500, Loss: 151.1825\n",
      "Epoch 299/500, Loss: 154.7971\n",
      "Epoch 300/500, Loss: 153.7537\n",
      "Epoch 301/500, Loss: 142.7480\n",
      "Epoch 302/500, Loss: 141.7908\n",
      "Epoch 303/500, Loss: 165.2280\n",
      "Epoch 304/500, Loss: 146.1067\n",
      "Epoch 305/500, Loss: 138.3608\n",
      "Epoch 306/500, Loss: 140.0024\n",
      "Epoch 307/500, Loss: 137.0974\n",
      "Epoch 308/500, Loss: 140.4262\n",
      "Epoch 309/500, Loss: 145.6983\n",
      "Epoch 310/500, Loss: 141.4063\n",
      "Epoch 311/500, Loss: 129.5353\n",
      "Epoch 312/500, Loss: 142.9730\n",
      "Epoch 313/500, Loss: 137.5347\n",
      "Epoch 314/500, Loss: 134.6206\n",
      "Epoch 315/500, Loss: 154.3118\n",
      "Epoch 316/500, Loss: 123.4336\n",
      "Epoch 317/500, Loss: 142.6284\n",
      "Epoch 318/500, Loss: 138.5207\n",
      "Epoch 319/500, Loss: 128.6826\n",
      "Epoch 320/500, Loss: 134.3106\n",
      "Epoch 321/500, Loss: 131.0300\n",
      "Epoch 322/500, Loss: 148.5313\n",
      "Epoch 323/500, Loss: 125.6926\n",
      "Epoch 324/500, Loss: 118.0900\n",
      "Epoch 325/500, Loss: 133.9608\n",
      "Epoch 326/500, Loss: 126.1522\n",
      "Epoch 327/500, Loss: 114.6998\n",
      "Epoch 328/500, Loss: 121.3885\n",
      "Epoch 329/500, Loss: 122.2458\n",
      "Epoch 330/500, Loss: 126.5858\n",
      "Epoch 331/500, Loss: 117.8868\n",
      "Epoch 332/500, Loss: 120.0593\n",
      "Epoch 333/500, Loss: 122.2807\n",
      "Epoch 334/500, Loss: 115.1088\n",
      "Epoch 335/500, Loss: 118.5445\n",
      "Epoch 336/500, Loss: 117.7680\n",
      "Epoch 337/500, Loss: 110.3045\n",
      "Epoch 338/500, Loss: 108.3708\n",
      "Epoch 339/500, Loss: 117.7671\n",
      "Epoch 340/500, Loss: 113.3723\n",
      "Epoch 341/500, Loss: 111.8842\n",
      "Epoch 342/500, Loss: 105.7111\n",
      "Epoch 343/500, Loss: 127.0813\n",
      "Epoch 344/500, Loss: 114.9550\n",
      "Epoch 345/500, Loss: 110.5387\n",
      "Epoch 346/500, Loss: 105.3647\n",
      "Epoch 347/500, Loss: 94.0785\n",
      "Epoch 348/500, Loss: 115.2582\n",
      "Epoch 349/500, Loss: 108.4167\n",
      "Epoch 350/500, Loss: 107.1320\n",
      "Epoch 351/500, Loss: 104.8886\n",
      "Epoch 352/500, Loss: 102.9879\n",
      "Epoch 353/500, Loss: 119.1920\n",
      "Epoch 354/500, Loss: 106.0338\n",
      "Epoch 355/500, Loss: 107.1123\n",
      "Epoch 356/500, Loss: 93.0440\n",
      "Epoch 357/500, Loss: 98.6439\n",
      "Epoch 358/500, Loss: 114.8142\n",
      "Epoch 359/500, Loss: 110.7542\n",
      "Epoch 360/500, Loss: 105.8364\n",
      "Epoch 361/500, Loss: 104.9548\n",
      "Epoch 362/500, Loss: 94.8912\n",
      "Epoch 363/500, Loss: 79.8060\n",
      "Epoch 364/500, Loss: 109.2357\n",
      "Epoch 365/500, Loss: 122.5694\n",
      "Epoch 366/500, Loss: 92.5128\n",
      "Epoch 367/500, Loss: 91.4743\n",
      "Epoch 368/500, Loss: 107.8951\n",
      "Epoch 369/500, Loss: 101.1811\n",
      "Epoch 370/500, Loss: 86.0185\n",
      "Epoch 371/500, Loss: 96.8225\n",
      "Epoch 372/500, Loss: 95.5381\n",
      "Epoch 373/500, Loss: 103.7453\n",
      "Epoch 374/500, Loss: 96.5165\n",
      "Epoch 375/500, Loss: 80.8479\n",
      "Epoch 376/500, Loss: 105.8437\n",
      "Epoch 377/500, Loss: 88.1292\n",
      "Epoch 378/500, Loss: 81.7593\n",
      "Epoch 379/500, Loss: 119.5309\n",
      "Epoch 380/500, Loss: 97.3616\n",
      "Epoch 381/500, Loss: 76.2864\n",
      "Epoch 382/500, Loss: 76.9994\n",
      "Epoch 383/500, Loss: 106.1068\n",
      "Epoch 384/500, Loss: 82.1166\n",
      "Epoch 385/500, Loss: 96.2439\n",
      "Epoch 386/500, Loss: 93.0004\n",
      "Epoch 387/500, Loss: 74.5860\n",
      "Epoch 388/500, Loss: 103.7997\n",
      "Epoch 389/500, Loss: 79.1378\n",
      "Epoch 390/500, Loss: 87.5860\n",
      "Epoch 391/500, Loss: 96.4598\n",
      "Epoch 392/500, Loss: 83.8254\n",
      "Epoch 393/500, Loss: 71.3317\n",
      "Epoch 394/500, Loss: 102.5366\n",
      "Epoch 395/500, Loss: 90.4188\n",
      "Epoch 396/500, Loss: 68.8655\n",
      "Epoch 397/500, Loss: 97.2128\n",
      "Epoch 398/500, Loss: 87.8402\n",
      "Epoch 399/500, Loss: 70.7966\n",
      "Epoch 400/500, Loss: 91.5319\n",
      "Epoch 401/500, Loss: 77.7617\n",
      "Epoch 402/500, Loss: 65.5156\n",
      "Epoch 403/500, Loss: 89.3456\n",
      "Epoch 404/500, Loss: 95.7365\n",
      "Epoch 405/500, Loss: 81.2782\n",
      "Epoch 406/500, Loss: 73.0574\n",
      "Epoch 407/500, Loss: 75.6816\n",
      "Epoch 408/500, Loss: 87.4041\n",
      "Epoch 409/500, Loss: 75.5847\n",
      "Epoch 410/500, Loss: 66.1409\n",
      "Epoch 411/500, Loss: 82.2980\n",
      "Epoch 412/500, Loss: 103.4842\n",
      "Epoch 413/500, Loss: 75.9537\n",
      "Epoch 414/500, Loss: 59.2084\n",
      "Epoch 415/500, Loss: 64.4299\n",
      "Epoch 416/500, Loss: 76.9842\n",
      "Epoch 417/500, Loss: 91.8476\n",
      "Epoch 418/500, Loss: 79.5755\n",
      "Epoch 419/500, Loss: 61.0048\n",
      "Epoch 420/500, Loss: 89.5829\n",
      "Epoch 421/500, Loss: 62.7320\n",
      "Epoch 422/500, Loss: 59.0683\n",
      "Epoch 423/500, Loss: 104.2208\n",
      "Epoch 424/500, Loss: 76.8464\n",
      "Epoch 425/500, Loss: 78.9960\n",
      "Epoch 426/500, Loss: 64.0964\n",
      "Epoch 427/500, Loss: 70.4096\n",
      "Epoch 428/500, Loss: 93.2827\n",
      "Epoch 429/500, Loss: 69.4169\n",
      "Epoch 430/500, Loss: 91.1149\n",
      "Epoch 431/500, Loss: 62.1715\n",
      "Epoch 432/500, Loss: 60.1393\n",
      "Epoch 433/500, Loss: 74.0856\n",
      "Epoch 434/500, Loss: 86.4237\n",
      "Epoch 435/500, Loss: 64.3690\n",
      "Epoch 436/500, Loss: 64.5033\n",
      "Epoch 437/500, Loss: 78.9521\n",
      "Epoch 438/500, Loss: 53.5043\n",
      "Epoch 439/500, Loss: 82.9015\n",
      "Epoch 440/500, Loss: 86.7180\n",
      "Epoch 441/500, Loss: 63.4407\n",
      "Epoch 442/500, Loss: 53.0722\n",
      "Epoch 443/500, Loss: 82.1842\n",
      "Epoch 444/500, Loss: 70.0906\n",
      "Epoch 445/500, Loss: 55.4309\n",
      "Epoch 446/500, Loss: 56.3113\n",
      "Epoch 447/500, Loss: 72.8037\n",
      "Epoch 448/500, Loss: 85.2112\n",
      "Epoch 449/500, Loss: 72.1441\n",
      "Epoch 450/500, Loss: 50.2630\n",
      "Epoch 451/500, Loss: 57.2280\n",
      "Epoch 452/500, Loss: 85.9819\n",
      "Epoch 453/500, Loss: 57.7826\n",
      "Epoch 454/500, Loss: 64.2983\n",
      "Epoch 455/500, Loss: 82.9003\n",
      "Epoch 456/500, Loss: 76.8091\n",
      "Epoch 457/500, Loss: 73.2892\n",
      "Epoch 458/500, Loss: 56.9609\n",
      "Epoch 459/500, Loss: 44.6746\n",
      "Epoch 460/500, Loss: 81.2012\n",
      "Epoch 461/500, Loss: 82.2479\n",
      "Epoch 462/500, Loss: 52.3800\n",
      "Epoch 463/500, Loss: 42.0905\n",
      "Epoch 464/500, Loss: 78.6057\n",
      "Epoch 465/500, Loss: 73.4760\n",
      "Epoch 466/500, Loss: 53.5850\n",
      "Epoch 467/500, Loss: 44.4640\n",
      "Epoch 468/500, Loss: 60.6223\n",
      "Epoch 469/500, Loss: 80.1369\n",
      "Epoch 470/500, Loss: 58.2702\n",
      "Epoch 471/500, Loss: 54.2000\n",
      "Epoch 472/500, Loss: 76.8640\n",
      "Epoch 473/500, Loss: 49.5738\n",
      "Epoch 474/500, Loss: 46.0717\n",
      "Epoch 475/500, Loss: 84.6336\n",
      "Epoch 476/500, Loss: 58.1646\n",
      "Epoch 477/500, Loss: 44.3397\n",
      "Epoch 478/500, Loss: 74.8752\n",
      "Epoch 479/500, Loss: 64.5497\n",
      "Epoch 480/500, Loss: 55.9973\n",
      "Epoch 481/500, Loss: 73.5250\n",
      "Epoch 482/500, Loss: 58.3851\n",
      "Epoch 483/500, Loss: 46.2547\n",
      "Epoch 484/500, Loss: 36.9709\n",
      "Epoch 485/500, Loss: 68.0112\n",
      "Epoch 486/500, Loss: 92.0391\n",
      "Epoch 487/500, Loss: 47.1972\n",
      "Epoch 488/500, Loss: 54.7466\n",
      "Epoch 489/500, Loss: 64.1822\n",
      "Epoch 490/500, Loss: 44.3654\n",
      "Epoch 491/500, Loss: 70.0912\n",
      "Epoch 492/500, Loss: 41.0014\n",
      "Epoch 493/500, Loss: 91.6115\n",
      "Epoch 494/500, Loss: 56.8547\n",
      "Epoch 495/500, Loss: 43.6404\n",
      "Epoch 496/500, Loss: 60.1288\n",
      "Epoch 497/500, Loss: 73.0135\n",
      "Epoch 498/500, Loss: 49.0408\n",
      "Epoch 499/500, Loss: 55.9512\n",
      "Epoch 500/500, Loss: 54.3941\n"
     ]
    }
   ],
   "source": [
    "# 6. Train the model\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, _ = model(batch_X)\n",
    "        loss = criterion(out, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79324dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), \"char_rnn_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86134ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate text\n",
    "def generate_text(model, start_seq, gen_length=300, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_seq = [char2idx.get(c, 0) for c in start_seq.lower()]\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated = start_seq\n",
    "\n",
    "    hidden = None\n",
    "    for _ in range(gen_length):\n",
    "        out, hidden = model(input_seq, hidden)\n",
    "        out = out.squeeze().div(temperature).exp()\n",
    "        prob = out / torch.sum(out)\n",
    "        next_idx = torch.multinomial(prob, 1).item()\n",
    "        next_char = idx2char[next_idx]\n",
    "        generated += next_char\n",
    "\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[next_idx]], device=device)], dim=1)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e172423",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be or not to be, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_text(\u001b[43mmodel\u001b[49m, seed, gen_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.11\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "seed = \"to be or not to be, \"\n",
    "print(generate_text(model, seed, gen_length=300, temperature=0.11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42c2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d6ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
